{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFjw5R8ENPlDvCVaBt9Jkq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/day-ch/Appteste/blob/main/O_que_se_tem_estudado_sobre_videogames_e_o_espectro_autista%3F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O que se tem estudado sobre videogames e o espectro autista? Uma revisão sistemática de dados da base pubmed com auxílio de processamento de linguagem natural e deep learning\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GxnHjoK7vQ5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook traz o código em Phython utilizado para a elaboração da revisão sistemática de dados do pubmed \"O que se tem estudado sobre videogames e o espectro autista: uma revisão sistemática de dados da base pubmed com auxílio de processamento de linguagem natural e deep learning\", apresentado no formato de poster no VII Congresso Internacional e XXVII Congresso Nacional da ABENEPI (Associação Brasileira de Neurologia e Psiquiatria Infantil e Profissões Afins)\n",
        "\n",
        "Os códigos podem ser utilizados em auxílio a pesquisas e revisões de outros temas na base pubmed.\n",
        "\n",
        "Para pesquisar no pubmed o tema de interesse ((\"Autism Spectrum Disorder\"[Mesh]) AND \"Video Games\"[Mesh]), baixar de forma automatizada os resultados da busca no formato de uma tabela (tabela com um artigo por linha com"
      ],
      "metadata": {
        "id": "TO8LfGxJxEk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RK2HTGFJzNOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) DOWNLOAD DOS ABSTRACTS\n",
        "\n",
        "\n",
        "> Inicialmente o tema de interesse é buscado na base pubmed e os dados de cada artigo encontrado são baixados e salvos em um tabela no formato '.csv'. São dois arquivos gerados, um com os artigos que tem 'abstracts' completos e um arquivo com 'abstracts' parciais.\n",
        "\n",
        "\n",
        "> Cada linha da tabela corresponde a um artigo, com colunas para título do artigo, autores, informações de autores, revista, 'abstract', DOI e miscelânea.\n",
        "\n",
        "\n",
        "> O algoritmo abaixo utiliza o pacote 'eutils 0.6.0' para esses procedimentos.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xavWv9V4wMUh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d917bd93",
        "outputId": "56b7f9bb-7128-4e29-b10d-dc506cf07ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the esearch command:\n",
            "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=((\"Autism+Spectrum+Disorder\"%5BMesh%5D)++AND+\"Video+Games\"%5BMesh%5D&usehistory=y&rettype=json\n",
            "\n",
            "this is efetch run number 1\n",
            "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_65341f44b2e2fd658c256d30&retstart=0&retmax=20&retmode=text&rettype=abstract\n",
            "a total of 20 abstracts have been downloaded.\n",
            "\n",
            "this is efetch run number 2\n",
            "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_65341f44b2e2fd658c256d30&retstart=20&retmax=20&retmode=text&rettype=abstract\n",
            "a total of 40 abstracts have been downloaded.\n",
            "\n",
            "this is efetch run number 3\n",
            "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_65341f44b2e2fd658c256d30&retstart=40&retmax=20&retmode=text&rettype=abstract\n",
            "a total of 60 abstracts have been downloaded.\n",
            "\n",
            "this is efetch run number 4\n",
            "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_65341f44b2e2fd658c256d30&retstart=60&retmax=20&retmode=text&rettype=abstract\n",
            "a total of 76 abstracts have been downloaded.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "import urllib\n",
        "from time import sleep\n",
        "\n",
        "query = '((\"Autism+Spectrum+Disorder\"%5BMesh%5D)++AND+\"Video+Games\"%5BMesh%5D'\n",
        "# common settings between esearch and efetch\n",
        "base_url = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\n",
        "db = 'db=pubmed'\n",
        "\n",
        "# esearch settings\n",
        "search_eutil = 'esearch.fcgi?'\n",
        "search_term = '&term=' + query\n",
        "search_usehistory = '&usehistory=y'\n",
        "search_rettype = '&rettype=json'\n",
        "\n",
        "# call the esearch command for the query and read the web result\n",
        "search_url = base_url+search_eutil+db+search_term+search_usehistory+search_rettype\n",
        "print(\"this is the esearch command:\\n\" + search_url + \"\\n\")\n",
        "f = urllib.request.urlopen (search_url)\n",
        "search_data = f.read().decode('utf-8')\n",
        "\n",
        "# extract the total abstract count\n",
        "total_abstract_count = int(re.findall(\"<Count>(\\d+?)</Count>\",search_data)[0])\n",
        "\n",
        "# efetch settings\n",
        "fetch_eutil = 'efetch.fcgi?'\n",
        "retmax = 20\n",
        "retstart = 0\n",
        "fetch_retmode = \"&retmode=text\"\n",
        "fetch_rettype = \"&rettype=abstract\"\n",
        "\n",
        "# obtain webenv and querykey settings from the esearch results\n",
        "fetch_webenv = \"&WebEnv=\" + re.findall (\"<WebEnv>(\\S+)<\\/WebEnv>\", search_data)[0]\n",
        "fetch_querykey = \"&query_key=\" + re.findall(\"<QueryKey>(\\d+?)</QueryKey>\",search_data)[0]\n",
        "\n",
        "# call efetch commands using a loop until all abstracts are obtained\n",
        "run = True\n",
        "all_abstracts = list()\n",
        "loop_counter = 1\n",
        "\n",
        "while run:\n",
        "    print(\"this is efetch run number \" + str(loop_counter))\n",
        "    loop_counter += 1\n",
        "    fetch_retstart = \"&retstart=\" + str(retstart)\n",
        "    fetch_retmax = \"&retmax=\" + str(retmax)\n",
        "    # create the efetch url\n",
        "    fetch_url = base_url+fetch_eutil+db+fetch_querykey+fetch_webenv+fetch_retstart+fetch_retmax+fetch_retmode+fetch_rettype\n",
        "    print(fetch_url)\n",
        "    # open the efetch url\n",
        "    f = urllib.request.urlopen (fetch_url)\n",
        "    fetch_data = f.read().decode('utf-8')\n",
        "    # split the data into individual abstracts\n",
        "    abstracts = fetch_data.split(\"\\n\\n\\n\")\n",
        "    # append to the list all_abstracts\n",
        "    all_abstracts = all_abstracts+abstracts\n",
        "    print(\"a total of \" + str(len(all_abstracts)) + \" abstracts have been downloaded.\\n\")\n",
        "    # wait 2 seconds so we don't get blocked\n",
        "    sleep(2)\n",
        "    # update retstart to download the next chunk of abstracts\n",
        "    retstart = retstart + retmax\n",
        "    if retstart > total_abstract_count:\n",
        "        run = False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"TCRabstracts.csv\", \"wt\",encoding=\"utf-8\") as abstracts_file:\n",
        "    abstract_writer = csv.writer(abstracts_file)\n",
        "    abstract_writer.writerow(['Journal', 'Title', 'Authors', 'Author_Information', 'Abstract', 'DOI', 'Misc'])\n",
        "    #For each abstract, split into categories and write it to the csv file\n",
        "    for abstract in all_abstracts:\n",
        "        #To obtain categories, split every double newline.\n",
        "        split_abstract = abstract.split(\"\\n\\n\")\n",
        "        abstract_writer.writerow(split_abstract)"
      ],
      "metadata": {
        "id": "veuk0e_2m1R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"TCRabstracts.csv\", \"wt\",encoding=\"utf-8\") as abstracts_file, open (\"partial_abstracts.csv\", \"wt\",encoding=\"utf-8\") as partial_abstracts:\n",
        "    # csv writer for full abstracts\n",
        "    abstract_writer = csv.writer(abstracts_file)\n",
        "    abstract_writer.writerow(['Journal', 'Title', 'Authors', 'Author_Information', 'Abstract', 'DOI', 'Misc'])\n",
        "    # csv writer for partial abstracts\n",
        "    partial_abstract_writer = csv.writer(partial_abstracts)\n",
        "    #For each abstract, split into categories and write it to the csv file\n",
        "    for abstract in all_abstracts:\n",
        "        #To obtain categories, split every double newline.\n",
        "        split_abstract = abstract.split(\"\\n\\n\")\n",
        "        if len(split_abstract) > 5:\n",
        "            abstract_writer.writerow(split_abstract)\n",
        "        else:\n",
        "            partial_abstract_writer.writerow(split_abstract)"
      ],
      "metadata": {
        "id": "tyKLoWnGm4Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "APLICAÇÃO DE CRITÉRIOS DE INCLUSÃO E EXCLUSÃO"
      ],
      "metadata": {
        "id": "6qoqo79Z32gA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tabela gerada no passo anterior é manualmente checada e os artigos sem dados para 'abstract' e aqueles que não se encaixam nos critérios de inclusão ou apresentam critérios para exclusão são excluídos da tabela.\n",
        "\n",
        "\n",
        "> Critérios de inclusão: População que engloba crianças, adolescentes ou adultos com transtorno do espectro autista (TEA). Artigos sobre uso de video games em português, inglês ou espanhol. Artigos dos últimos 20 anos.\n",
        "\n",
        "\n",
        "> Critérios de exclusão: População que não TEA. Outras línguas que não português, inglês ou espanhol. Artigos que não tem como assunto video games. Artigos sem abstracts disponíveis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DqPFKGbM2PhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUMARIZAÇÃO DOS ABSTRACTS UTILIZANDO DEEP LEARNING"
      ],
      "metadata": {
        "id": "vbzHBKC_4-Pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os abstracts são sumarizados de forma automatizada utilizando um modelo de deep learning de arquitetura 'transformers' da google o 'Pegasus x-sum'. Os abstracts sumarizados em uma frase são salvos em um arquivo/tabela .csv.  "
      ],
      "metadata": {
        "id": "RBhKUYa_5MsP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAhmNI-kLtxu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#leitura de csv com titulo e abstract\n",
        "df=pd.read_csv('/content/drive/MyDrive/TRABALHO ABENAPI/1TEAgames-abstracts.csv', usecols= ['Title','Abstract'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6YiD0cQnnGY",
        "outputId": "59c6f643-60c1-46bd-9d93-5d2fd20cc540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Title  \\\n",
            "0   A preliminary study into internet related addi...   \n",
            "1   Video Game Use, Aggression, and Social Impairm...   \n",
            "2   I can actually do it without any help or someo...   \n",
            "3   [Formula: see text]Is active video gaming asso...   \n",
            "4   A Mobile Game Platform for Improving Social Co...   \n",
            "..                                                ...   \n",
            "70  Using computerized games to teach face recogni...   \n",
            "71  Designing affective video games to support the...   \n",
            "72  Engagement with electronic screen media among ...   \n",
            "73  A Virtual Reality Training Application for Adu...   \n",
            "74                             Helping children play.   \n",
            "\n",
            "                                             Abstract  \n",
            "0   In recent decades, studies have investigated a...  \n",
            "1   We used parent report data to investigate vide...  \n",
            "2   Research into autistic adolescents' engagement...  \n",
            "3   Active video gaming (AVG) is a way that childr...  \n",
            "4   BACKGROUND: Many children with autism cannot r...  \n",
            "..                                                ...  \n",
            "70  BACKGROUND: An emerging body of evidence indic...  \n",
            "71  Autism spectrum disorders (ASD) are a group of...  \n",
            "72  This study investigated the relative engagemen...  \n",
            "73  Asperger's syndrome is a disorder that involve...  \n",
            "74                                                NaN  \n",
            "\n",
            "[75 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text']=df[\"Title\"].map(str) + \" \" + df[\"Abstract\"]\n",
        "text=df['Text'].tolist()\n",
        "text2=[]\n",
        "for item in text:\n",
        "    item2=str(item)\n",
        "    text2.append(item2)"
      ],
      "metadata": {
        "id": "uBsC7zfHnqmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[sentencepiece]\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VSGrKIfn3aJ",
        "outputId": "da198ae8-0027-48bc-ab7f-9e0344f936e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.1)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "\n",
        "src_text = text2\n",
        "\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "summary=[]\n",
        "for text in src_text:\n",
        "  batch = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
        "  translated = model.generate(**batch)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  summary.append(tgt_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-z8Cv4xn4R9",
        "outputId": "37986c4a-c2e6-4176-b5f5-dd3cbf580b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myFile = open('demo_file.csv', 'w')\n",
        "writer = csv.writer(myFile)\n",
        "writer.writerow(['Sumario'])\n",
        "for data_list in summary:\n",
        "    writer.writerow(data_list)\n",
        "myFile.close()\n",
        "myFile = open('demo_file.csv', 'r')\n",
        "print(\"The content of the csv file is:\")\n",
        "print(myFile.read())\n",
        "myFile.close()"
      ],
      "metadata": {
        "id": "GjjAqpqloKKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROCESSAMENTO DE LINGUAGEM NATURAL E FREQUÊNCIAS"
      ],
      "metadata": {
        "id": "NcfquwbYD2yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "São utilizadas técnicas de processamento de linguagem natural aplicadas as colunas da tabela dos artigos. São calculados as frequências das palavras na coluna Autores para auxiliar na identificação dos autores com o maior número de publicações sobre o tema. O mesmo é feito para a coluna 'Journal', 'Informação\n",
        "de autores', título e abstracts.\n",
        "\n",
        "A partir da frequência de palavras da coluna 'abstract' (com a exclusão de stopwords definidas) é montado uma nuvem de palavras que pode apontar para temas mais investigados pelos artigos pubmed sobre video games e o espectro autista.\n",
        "\n"
      ],
      "metadata": {
        "id": "eRVZ0_NND-Xz"
      }
    }
  ]
}