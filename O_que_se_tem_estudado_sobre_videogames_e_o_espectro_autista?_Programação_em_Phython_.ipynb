{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1rSctEicnkibTGpCaD52al7YZtjkRt3gy",
      "authorship_tag": "ABX9TyNlxapUBbrRJRl2B79zn+V4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/day-ch/Appteste/blob/main/O_que_se_tem_estudado_sobre_videogames_e_o_espectro_autista%3F_Programa%C3%A7%C3%A3o_em_Phython_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O que se tem estudado sobre videogames e o espectro autista? Uma revisão sistemática de dados da base pubmed com auxílio de processamento de linguagem natural e deep learning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GxnHjoK7vQ5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook traz o código em Phython utilizado para a elaboração da revisão sistemática de dados do pubmed \"O que se tem estudado sobre videogames e o espectro autista: uma revisão sistemática de dados da base pubmed com auxílio de processamento de linguagem natural e deep learning\", apresentado no formato de poster no VII Congresso Internacional e XXVII Congresso Nacional da ABENEPI (Associação Brasileira de Neurologia e Psiquiatria Infantil e Profissões Afins).\n",
        "\n",
        "Os códigos podem ser utilizados em auxílio a pesquisas e revisões de outros temas na base pubmed.\n"
      ],
      "metadata": {
        "id": "TO8LfGxJxEk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) DOWNLOAD DOS ABSTRACTS\n",
        "\n",
        "\n",
        "> Inicialmente o tema de interesse é buscado na base pubmed e os dados de cada artigo encontrado são baixados e salvos em um tabela no formato '.csv'. É gerado um arquivo com os artigos e seus 'abstracts'.\n",
        "\n",
        "> Busca-se pelos termos pretendidos no pubmed, após a busca copia-se o que aparece no campo de endereço eletrônico após o \"term=\" e cola esse código no paramêtro \"query\" do código abaixo.\n",
        "\n",
        "> O código abaixo copia todas as informações da busca para uma tabela .csv que fica salva na aba 'arquivos' do colab.\n",
        "\n",
        "> Cada linha da tabela corresponde a um artigo, com colunas para título do artigo, autores, informações de autores, revista, 'abstract', DOI e miscelânea.\n",
        "\n",
        "> O algoritmo abaixo utiliza o pacote 'eutils 0.6.0' para esses procedimentos.\n",
        "\n",
        "\n",
        "\n",
        "Para mais informações sobre os parâmetros e o uso do pacote eutils para baixar informações do pubmed favor consultar:\n",
        "\n",
        " Sayers E. The E-utilities In-Depth: Parameters, Syntax and More. 2009 May 29 [Updated 2022 Nov 30]. In: Entrez Programming Utilities Help [Internet]. Bethesda (MD): National Center for Biotechnology Information (US); 2010-. Available from: https://www.ncbi.nlm.nih.gov/books/NBK25499/https://www.ncbi.nlm.nih.gov/books/NBK25499/\n",
        "\n",
        " Sayers E. E-utilities Quick Start. 2008 Dec 12 [Updated 2018 Oct 24]. In: Entrez Programming Utilities Help [Internet]. Bethesda (MD): National Center for Biotechnology Information (US); 2010-. Available from: https://www.ncbi.nlm.nih.gov/books/NBK25500/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xavWv9V4wMUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import urllib\n",
        "from time import sleep\n",
        "\n",
        "# inicialmente busca-se as palavras chaves e/ou termos no campo de busca do pubmed\n",
        "#(no caso da nossa revisão: (\"Autism Spectrum Disorder\"[Mesh]) AND \"Video Games\"[Mesh])), clica em 'Search'\n",
        "# vai aparecer no campo de endereço eletrônico :\n",
        "# https://pubmed.ncbi.nlm.nih.gov/?sort=date&term=(%22Autism+Spectrum+Disorder%22%5BMesh%5D)+AND+%22Video+Games%22%5BMesh%5D\n",
        "# copia tudo que aparece após 'term=', (%22Autism+Spectrum+Disorder%22%5BMesh%5D)+AND+%22Video+Games%22%5BMesh%5D\n",
        "# coloca como o parâmetro query abaixo\n",
        "\n",
        "query = '((\"Autism+Spectrum+Disorder\"%5BMesh%5D)++AND+\"Video+Games\"%5BMesh%5D'\n",
        "# common settings between esearch and efetch\n",
        "base_url = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\n",
        "db = 'db=pubmed'\n",
        "\n",
        "# esearch settings\n",
        "search_eutil = 'esearch.fcgi?'\n",
        "search_term = '&term=' + query\n",
        "search_usehistory = '&usehistory=y'\n",
        "search_rettype = '&rettype=json'\n",
        "\n",
        "# call the esearch command for the query and read the web result\n",
        "search_url = base_url+search_eutil+db+search_term+search_usehistory+search_rettype\n",
        "print(\"this is the esearch command:\\n\" + search_url + \"\\n\")\n",
        "f = urllib.request.urlopen (search_url)\n",
        "search_data = f.read().decode('utf-8')\n",
        "\n",
        "# extract the total abstract count\n",
        "total_abstract_count = int(re.findall(\"<Count>(\\d+?)</Count>\",search_data)[0])\n",
        "\n",
        "# efetch settings\n",
        "fetch_eutil = 'efetch.fcgi?'\n",
        "retmax = 20\n",
        "retstart = 0\n",
        "fetch_retmode = \"&retmode=text\"\n",
        "fetch_rettype = \"&rettype=abstract\"\n",
        "\n",
        "# obtain webenv and querykey settings from the esearch results\n",
        "fetch_webenv = \"&WebEnv=\" + re.findall (\"<WebEnv>(\\S+)<\\/WebEnv>\", search_data)[0]\n",
        "fetch_querykey = \"&query_key=\" + re.findall(\"<QueryKey>(\\d+?)</QueryKey>\",search_data)[0]\n",
        "\n",
        "# call efetch commands using a loop until all abstracts are obtained\n",
        "run = True\n",
        "all_abstracts = list()\n",
        "loop_counter = 1\n",
        "\n",
        "while run:\n",
        "    print(\"this is efetch run number \" + str(loop_counter))\n",
        "    loop_counter += 1\n",
        "    fetch_retstart = \"&retstart=\" + str(retstart)\n",
        "    fetch_retmax = \"&retmax=\" + str(retmax)\n",
        "    # create the efetch url\n",
        "    fetch_url = base_url+fetch_eutil+db+fetch_querykey+fetch_webenv+fetch_retstart+fetch_retmax+fetch_retmode+fetch_rettype\n",
        "    print(fetch_url)\n",
        "    # open the efetch url\n",
        "    f = urllib.request.urlopen (fetch_url)\n",
        "    fetch_data = f.read().decode('utf-8')\n",
        "    # split the data into individual abstracts\n",
        "    abstracts = fetch_data.split(\"\\n\\n\\n\")\n",
        "    # append to the list all_abstracts\n",
        "    all_abstracts = all_abstracts+abstracts\n",
        "    print(\"a total of \" + str(len(all_abstracts)) + \" abstracts have been downloaded.\\n\")\n",
        "    # wait 2 seconds so we don't get blocked\n",
        "    sleep(2)\n",
        "    # update retstart to download the next chunk of abstracts\n",
        "    retstart = retstart + retmax\n",
        "    if retstart > total_abstract_count:\n",
        "        run = False"
      ],
      "metadata": {
        "id": "dKuBDquyFgU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Abstracts_total1.csv\", \"wt\",encoding=\"utf-8\") as abstracts_file:\n",
        "    abstract_writer = csv.writer(abstracts_file)\n",
        "    abstract_writer.writerow(['Journal', 'Title', 'Authors', 'Author_Information', 'Abstract', 'DOI', 'Misc'])\n",
        "    #For each abstract, split into categories and write it to the csv file\n",
        "    for abstract in all_abstracts:\n",
        "        #To obtain categories, split every double newline.\n",
        "        split_abstract = abstract.split(\"\\n\\n\")\n",
        "        abstract_writer.writerow(split_abstract)"
      ],
      "metadata": {
        "id": "veuk0e_2m1R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) APLICAÇÃO DE CRITÉRIOS DE INCLUSÃO E EXCLUSÃO"
      ],
      "metadata": {
        "id": "6qoqo79Z32gA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tabela gerada no passo anterior é manualmente checada. Os artigos que não se encaixam nos critérios de inclusão ou aqueles que apresentam critérios para exclusão são excluídos da tabela.\n",
        "\n",
        "\n",
        "> Critérios de inclusão: População que engloba crianças, adolescentes ou adultos com transtorno do espectro autista (TEA). Artigos sobre uso de video games em português, inglês ou espanhol. Artigos dos últimos 20 anos.\n",
        "\n",
        "\n",
        "> Critérios de exclusão: População que não TEA. Outras línguas que não português, inglês ou espanhol. Artigos que não tem como assunto video games. Artigos sem abstracts disponíveis.\n",
        "\n",
        "A tabela gerada após aplicação dos critérios de elegibilidade se encontra abaixo:\n",
        "\n",
        "https://drive.google.com/file/d/1ZQ_Gi8gS768-Zvj7t_fx2h-bepTvpG1a/view?usp=drive_link\n",
        "\n"
      ],
      "metadata": {
        "id": "DqPFKGbM2PhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) SUMARIZAÇÃO DOS ABSTRACTS UTILIZANDO DEEP LEARNING"
      ],
      "metadata": {
        "id": "vbzHBKC_4-Pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os abstracts são sumarizados de forma automatizada utilizando um modelo de deep learning de arquitetura 'transformers' da google o 'Pegasus x-sum'. Os abstracts sumarizados em uma frase são salvos em um arquivo/tabela .csv.\n",
        "\n"
      ],
      "metadata": {
        "id": "RBhKUYa_5MsP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAhmNI-kLtxu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#leitura de csv com titulo e abstract\n",
        "df=pd.read_csv('', usecols= ['Title','Abstract'])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "3xXPQW7NFlpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text']=df[\"Title\"].map(str) + \" \" + df[\"Abstract\"]\n",
        "text=df['Text'].tolist()\n",
        "text2=[]\n",
        "for item in text:\n",
        "    item2=str(item)\n",
        "    text2.append(item2)"
      ],
      "metadata": {
        "id": "uBsC7zfHnqmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[sentencepiece]\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "M-3VHYF4FogN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "\n",
        "src_text = text2\n",
        "\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "summary=[]\n",
        "for text in src_text:\n",
        "  batch = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
        "  translated = model.generate(**batch)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  summary.append(tgt_text)"
      ],
      "metadata": {
        "id": "UNzaiLX6FsUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myFile = open('Sumario.csv', 'w')\n",
        "writer = csv.writer(myFile)\n",
        "writer.writerow(['Sumario'])\n",
        "for data_list in summary:\n",
        "    writer.writerow(data_list)\n",
        "myFile.close()\n",
        "myFile = open('Sumario.csv', 'r')\n",
        "print(\"The content of the csv file is:\")\n",
        "print(myFile.read())\n",
        "myFile.close()"
      ],
      "metadata": {
        "id": "KrmuBQjjF072"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) PROCESSAMENTO DE LINGUAGEM NATURAL E FREQUÊNCIAS"
      ],
      "metadata": {
        "id": "NcfquwbYD2yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tabela gerada no passo 2 é pré-processada, sendo acrescentadas as colunas \"Ano\" e \"Journals\", conforme link abaixo:\n",
        "\n",
        "https://drive.google.com/file/d/1qrShMjjZhyfqnvGwGX0A4KrTnW5Z8eDa/view?usp=drive_link\n",
        "\n",
        "São utilizadas técnicas de processamento de linguagem natural aplicadas as colunas da tabela dos artigos. São calculados as frequências das palavras na coluna Autores para auxiliar na identificação dos autores com o maior número de publicações sobre o tema. O mesmo é feito para a coluna 'Ano','Journal', 'Informação de autores' e 'abstracts'.\n",
        "\n",
        "A partir da frequência de palavras de todo o texto contido em todas as linhas da coluna 'abstract' (com a exclusão de stopwords definidas) é montado uma nuvem de palavras que pode apontar para temas mais investigados pelos artigos pubmed sobre video games e o espectro autista.\n",
        "\n"
      ],
      "metadata": {
        "id": "eRVZ0_NND-Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dr = pd.read_csv('/content/drive/MyDrive/COMPARTILHADA Poster Abenepi/Artigos_Revisão3.csv')"
      ],
      "metadata": {
        "id": "AIGlehq_Ag7i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FREQUÊNCIA AUTORES"
      ],
      "metadata": {
        "id": "AEDzm2tcBDmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dr[\"mytext_new\"] = dr['Authors'].str.lower().str.replace('[^\\w\\s]','').str.replace('\\n', ' ').str.replace('[^A-Za-z0-9]+', ' ')\n",
        "dr['Authors'] = dr['mytext_new'].str.replace('\\d+', '')\n",
        "\n",
        "new_df = dr.mytext_new.str.split(expand=True).stack().value_counts().reset_index()\n",
        "new_df.columns = ['Word', 'Frequency']\n",
        "\n",
        "mask = (new_df['Word'].str.len() > 3)\n",
        "dy = new_df.loc[mask]\n",
        "dy"
      ],
      "metadata": {
        "id": "ymIZX83OF58X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FREQUÊNCIA ANO"
      ],
      "metadata": {
        "id": "PKuBTu8_BHXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ano= dr['Ano'].value_counts()\n",
        "print(ano)"
      ],
      "metadata": {
        "id": "zXMFsqUvGDGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FREQUÊNCIA REVISTA"
      ],
      "metadata": {
        "id": "zfm9uPKZBX-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfj= dr['Journals'].value_counts()\n",
        "print(dfj)"
      ],
      "metadata": {
        "id": "gOu_oJAtGFHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FREQUÊNCIA DE PALAVRAS EM ABSTRACTS"
      ],
      "metadata": {
        "id": "zz2xlfUlBhNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Baixar o corpus de Stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Baixar o tokenizador\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Função para pré-processar o texto\n",
        "def preprocess_text(text):\n",
        "    # Converter para minúscula\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remover pontuações\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenizar o texto\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remover as stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    new_stopwords = [\"game\",\"games\",\"video\",\"years\",\"asd\", \"autism\", \"spectrum\",\"research\",\"disorders\",\"analysis\",\"people\",\"time\",\"studies\",\"findings\",\"control\",\"significant\",\"disorder\",\"analisys\",\"methods\",\"also\",\"associated\",\"disorder\", \"study\",\"results\", \"use\",\"individuals\",\"used\",\"data\",\"information\",\"autistic\",\"may\",\"group\", \"participants\", \"using\"]\n",
        "    stop_words.extend(new_stopwords)\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Juntar os tokens em um 'string'\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Aplicar a função de pré-processamento de texto e criar uma coluna com os abstracts processados\n",
        "dr['AbstractPre'] = dr['Abstract'].apply(preprocess_text)\n",
        "\n",
        "# Visualização das cinco primeiras linhas do dataframe pós-processamento\n",
        "print(dr.head())"
      ],
      "metadata": {
        "id": "-QjgaWCWGJzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Criando frequência de palavras\n",
        "word_freq = Counter()\n",
        "\n",
        "for text in dr['AbstractPre']:\n",
        "    word_freq.update(text.split())\n",
        "\n",
        "# Visualizar palavras mais frequentes\n",
        "print(word_freq.most_common(30))"
      ],
      "metadata": {
        "id": "yxFXp8mlGMti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gerar uma nuvem de palavras\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='black').generate_from_frequencies(word_freq)\n",
        "\n",
        "# Plotar uma nuvem de palavras\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Y6vcogGGPiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create image as cloud\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "# store to file\n",
        "plt.savefig(\"cloud.png\", format=\"png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "66L_-up6GTPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "mask = np.array(Image.open(\"figura\"))\n",
        "mask=np.where(mask > 3, 255, mask)\n",
        "wordcloud = WordCloud(background_color='white',mask=mask,contour_color='#023075',contour_width=3,colormap='rainbow',min_font_size=10).generate_from_frequencies(word_freq)\n",
        "\n",
        "\n",
        "# create image as cloud\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "# store to file\n",
        "plt.savefig(\"cloud.png\", format=\"png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gocUX5dtGVwg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}